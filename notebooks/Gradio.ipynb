{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResMLP",
      "provenance": [],
      "authorship_tag": "ABX9TyP1710V+maPCdLg5k/8lUIc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SauravMaheshkar/ResMLP-Flax/blob/main/notebooks/Gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s227ZnS0x1ab"
      },
      "source": [
        "\n",
        "# 🏗 Basic Setup\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ET_FB6IDk4pz"
      },
      "source": [
        "%%capture\n",
        "!pip install --upgrade gradio\n",
        "!pip install timm -upgrade\n",
        "!pip install git+https://github.com/rwightman/pytorch-image-models.git\n",
        "!pip install submitit\n",
        "!pip install --upgrade wandb"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k58oK72AwQgP"
      },
      "source": [
        "!wandb login"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vveHSu8wlH-H"
      },
      "source": [
        "import torch\n",
        "import requests\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from timm.models.layers import trunc_normal_,  DropPath\n",
        "from timm.models.vision_transformer import Mlp, PatchEmbed\n",
        "\n",
        "torch.set_grad_enabled(False);"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfexUdbbx7Qt"
      },
      "source": [
        "\n",
        "# 🏠 Model Architecture\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWV36f3tn751"
      },
      "source": [
        "class Affine(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.alpha = nn.Parameter(torch.ones(dim))\n",
        "        self.beta = nn.Parameter(torch.zeros(dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.alpha * x + self.beta    \n",
        "    \n",
        "class layers_scale_mlp_blocks(nn.Module):\n",
        "\n",
        "    def __init__(self, dim, drop=0., drop_path=0., act_layer=nn.GELU,init_values=1e-4,num_patches = 196):\n",
        "        super().__init__()\n",
        "        self.norm1 = Affine(dim)\n",
        "        self.attn = nn.Linear(num_patches, num_patches)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = Affine(dim)\n",
        "        self.mlp = Mlp(in_features=dim, hidden_features=int(4.0 * dim), act_layer=act_layer, drop=drop)\n",
        "        self.gamma_1 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
        "        self.gamma_2 = nn.Parameter(init_values * torch.ones((dim)),requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.gamma_1 * self.attn(self.norm1(x).transpose(1,2)).transpose(1,2))\n",
        "        x = x + self.drop_path(self.gamma_2 * self.mlp(self.norm2(x)))\n",
        "        return x \n",
        "\n",
        "\n",
        "class resmlp_models(nn.Module):\n",
        "\n",
        "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,drop_rate=0.,\n",
        "                 Patch_layer=PatchEmbed,act_layer=nn.GELU,\n",
        "                drop_path_rate=0.0,init_scale=1e-4):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "\n",
        "        self.num_classes = num_classes\n",
        "        self.num_features = self.embed_dim = embed_dim  \n",
        "\n",
        "        self.patch_embed = Patch_layer(\n",
        "                img_size=img_size, patch_size=patch_size, in_chans=int(in_chans), embed_dim=embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "        dpr = [drop_path_rate for i in range(depth)]\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            layers_scale_mlp_blocks(\n",
        "                dim=embed_dim,drop=drop_rate,drop_path=dpr[i],\n",
        "                act_layer=act_layer,init_values=init_scale,\n",
        "                num_patches=num_patches)\n",
        "            for i in range(depth)])\n",
        "\n",
        "\n",
        "        self.norm = Affine(embed_dim)\n",
        "\n",
        "\n",
        "\n",
        "        self.feature_info = [dict(num_chs=embed_dim, reduction=0, module='head')]\n",
        "        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=0.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "\n",
        "\n",
        "    def get_classifier(self):\n",
        "        return self.head\n",
        "\n",
        "    def reset_classifier(self, num_classes, global_pool=''):\n",
        "        self.num_classes = num_classes\n",
        "        self.head = nn.Linear(self.embed_dim, num_classes) if num_classes > 0 else nn.Identity()\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        B = x.shape[0]\n",
        "\n",
        "        x = self.patch_embed(x)\n",
        "\n",
        "        for i , blk in enumerate(self.blocks):\n",
        "            x  = blk(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        x = x.mean(dim=1).reshape(B,1,-1)\n",
        "\n",
        "        return x[:, 0]\n",
        "\n",
        "    def forward(self, x):\n",
        "        x  = self.forward_features(x)\n",
        "        x = self.head(x)\n",
        "        return x \n",
        "\n",
        "\n",
        "model = resmlp_models(\n",
        "        patch_size=8, embed_dim=768, depth=24,\n",
        "        Patch_layer=PatchEmbed,\n",
        "        init_scale=1e-6)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxxDOM97lfxd"
      },
      "source": [
        "checkpoint = torch.hub.load_state_dict_from_url(\n",
        "            url='https://dl.fbaipublicfiles.com/deit/resmlpB_24_no_dist.pth',\n",
        "            map_location=\"cpu\", check_hash=True\n",
        "        )\n",
        "            \n",
        "model.load_state_dict(checkpoint)\n",
        "model.eval();"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CT_uKSV6xvys"
      },
      "source": [
        "mean, std = [0.485, 0.456, 0.406],[0.229, 0.224, 0.225]    \n",
        "transformations = {}\n",
        "Rs_size=int(224/0.9)\n",
        "transform= transforms.Compose(\n",
        "        [transforms.Resize(Rs_size, interpolation=3),\n",
        "         transforms.CenterCrop(224),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean, std)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcm9YJRTruHE"
      },
      "source": [
        "# Download human-readable labels for ImageNet.\n",
        "response = requests.get(\"https://git.io/JJkYN\")\n",
        "labels = response.text.split(\"\\n\")\n",
        "\n",
        "def predict(inp):\n",
        "  inp = Image.fromarray(inp.astype('uint8'), 'RGB')\n",
        "  inp = transform(inp).unsqueeze(0)\n",
        "  with torch.no_grad():\n",
        "    prediction = torch.nn.functional.softmax(model(inp), dim=-1)[0]\n",
        "  return {labels[i]: float(prediction[i]) for i in range(1000)}"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5j93NnwEyBeE"
      },
      "source": [
        "# Gradio Demo\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA41opBLwBwQ"
      },
      "source": [
        "import wandb\n",
        "run = wandb.init(project='ResMLP', entity='sauravmaheshkar')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "-Wy8N2-BrgSq",
        "outputId": "75cb90c2-a3ee-403b-90a0-4205d4257628"
      },
      "source": [
        "import gradio as gr\n",
        "\n",
        "title = \"ResMLP-B24/8 Demo\"\n",
        "article = \"<p style='text-align: center'><a href='https://wandb.ai/sauravmaheshkar/ResMLP/reports/Feedforward-networks-for-image-classification--Vmlldzo4NTk3MDA'>Weights and Biases Report: Feedforward networks for image classification</a></p>\"\n",
        "\n",
        "inputs = gr.inputs.Image()\n",
        "outputs = gr.outputs.Label(num_top_classes=3)\n",
        "io = gr.Interface(fn=predict, inputs=inputs, outputs=outputs, title=title, article = article)\n",
        "io.launch(share=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "This share link will expire in 24 hours. If you need a permanent link, visit: https://gradio.app/introducing-hosted (NEW!)\n",
            "Running on External URL: https://37223.gradio.app\n",
            "Interface loading below...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"900\"\n",
              "            height=\"500\"\n",
              "            src=\"https://37223.gradio.app\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "        ></iframe>\n",
              "        "
            ],
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7f5206b99610>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<Flask 'gradio.networking'>,\n",
              " 'http://127.0.0.1:7866/',\n",
              " 'https://37223.gradio.app')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GbcPvEawfRv"
      },
      "source": [
        "io.integrate(wandb=wandb)\n",
        "run.finish()"
      ],
      "execution_count": 22,
      "outputs": []
    }
  ]
}